"""
API å“åº”æµ‹è¯•çš„ pytest é…ç½®

ä¸“é—¨ç”¨äºAPIå“åº”æµ‹è¯•çš„é…ç½®æ–‡ä»¶ã€‚
"""

import pytest
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

def pytest_configure(config):
    """pytesté…ç½®"""
    # æ³¨å†Œè‡ªå®šä¹‰æ ‡è®°
    config.addinivalue_line(
        "markers", "api_response: mark test as API response test"
    )
    config.addinivalue_line(
        "markers", "auth: mark test as authentication related"
    )
    config.addinivalue_line(
        "markers", "cases: mark test as cases related"
    )
    config.addinivalue_line(
        "markers", "knowledge: mark test as knowledge related"
    )
    config.addinivalue_line(
        "markers", "system: mark test as system related"
    )
    config.addinivalue_line(
        "markers", "development: mark test as development tools related"
    )
    config.addinivalue_line(
        "markers", "slow: mark test as slow running"
    )

def pytest_collection_modifyitems(config, items):
    """ä¿®æ”¹æµ‹è¯•é¡¹ç›®æ”¶é›†"""
    for item in items:
        # ä¸ºæ‰€æœ‰APIæµ‹è¯•æ·»åŠ æ ‡è®°
        if "test_api" in item.nodeid or "api" in item.parent.name:
            item.add_marker(pytest.mark.api_response)

        # æ ¹æ®æ–‡ä»¶åæ·»åŠ æ¨¡å—æ ‡è®°
        if "auth" in item.parent.name:
            item.add_marker(pytest.mark.auth)
        elif "cases" in item.parent.name:
            item.add_marker(pytest.mark.cases)
        elif "knowledge" in item.parent.name:
            item.add_marker(pytest.mark.knowledge)
        elif "system" in item.parent.name:
            item.add_marker(pytest.mark.system)
        elif "development" in item.parent.name:
            item.add_marker(pytest.mark.development)

        # æ ‡è®°æ…¢é€Ÿæµ‹è¯•
        if "performance" in item.name or "large" in item.name:
            item.add_marker(pytest.mark.slow)

@pytest.fixture(scope="session", autouse=True)
def setup_test_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    # ç¡®ä¿æµ‹è¯•ç›®å½•å­˜åœ¨
    test_results_dir = os.path.join(project_root, 'test-results')
    os.makedirs(test_results_dir, exist_ok=True)

    # ç¡®ä¿è¦†ç›–ç‡æŠ¥å‘Šç›®å½•å­˜åœ¨
    htmlcov_dir = os.path.join(project_root, 'htmlcov', 'api_responses')
    os.makedirs(htmlcov_dir, exist_ok=True)

    print("ğŸ”§ APIå“åº”æµ‹è¯•ç¯å¢ƒå·²å‡†å¤‡å°±ç»ª")

def pytest_sessionstart(session):
    """æµ‹è¯•ä¼šè¯å¼€å§‹"""
    print("ğŸš€ å¼€å§‹APIå“åº”æµ‹è¯•ä¼šè¯")

def pytest_sessionfinish(session, exitstatus):
    """æµ‹è¯•ä¼šè¯ç»“æŸ"""
    if exitstatus == 0:
        print("ğŸ‰ APIå“åº”æµ‹è¯•ä¼šè¯æˆåŠŸå®Œæˆ")
    else:
        print("âŒ APIå“åº”æµ‹è¯•ä¼šè¯å­˜åœ¨å¤±è´¥")

def pytest_runtest_logstart(nodeid, location):
    """æµ‹è¯•å¼€å§‹å‰çš„æ—¥å¿—"""
    pass

def pytest_runtest_logfinish(nodeid, location):
    """æµ‹è¯•ç»“æŸåçš„æ—¥å¿—"""
    pass

def pytest_terminal_summary(terminalreporter, exitstatus, config):
    """ç»ˆç«¯æ‘˜è¦"""
    terminalreporter.write_sep("=", "APIå“åº”æµ‹è¯•æ€»ç»“")

    if hasattr(terminalreporter.stats, 'passed'):
        passed = len(terminalreporter.stats.get('passed', []))
        terminalreporter.write_line(f"âœ… é€šè¿‡æµ‹è¯•: {passed}")

    if hasattr(terminalreporter.stats, 'failed'):
        failed = len(terminalreporter.stats.get('failed', []))
        if failed > 0:
            terminalreporter.write_line(f"âŒ å¤±è´¥æµ‹è¯•: {failed}")

    if hasattr(terminalreporter.stats, 'error'):
        errors = len(terminalreporter.stats.get('error', []))
        if errors > 0:
            terminalreporter.write_line(f"ğŸ”¥ é”™è¯¯æµ‹è¯•: {errors}")

    if hasattr(terminalreporter.stats, 'skipped'):
        skipped = len(terminalreporter.stats.get('skipped', []))
        if skipped > 0:
            terminalreporter.write_line(f"â­ï¸  è·³è¿‡æµ‹è¯•: {skipped}")

    terminalreporter.write_line("ğŸ“Š è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹ htmlcov/api_responses/index.html")
